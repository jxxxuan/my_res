{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jxxxuan/my_res/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqw2FQ-rh2p_"
      },
      "outputs": [],
      "source": [
        "import  tensorflow as tf\n",
        "from    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "from \ttensorflow import keras\n",
        "import  os\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    # [0~255] => [-1~1]\n",
        "    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "    return x,y\n",
        "\n",
        "\n",
        "batchsz = 128\n",
        "# [50k, 32, 32, 3], [10k, 1]\n",
        "(x, y), (x_val, y_val) = datasets.cifar10.load_data()\n",
        "y = tf.squeeze(y)\n",
        "y_val = tf.squeeze(y_val)\n",
        "y = tf.one_hot(y, depth=10) # [50k, 10]\n",
        "y_val = tf.one_hot(y_val, depth=10) # [10k, 10]\n",
        "print('datasets:', x.shape, y.shape, x_val.shape, y_val.shape, x.min(), x.max())\n",
        "\n",
        "\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "train_db = train_db.map(preprocess).shuffle(10000).batch(batchsz)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "test_db = test_db.map(preprocess).batch(batchsz)\n",
        "\n",
        "\n",
        "sample = next(iter(train_db))\n",
        "print('batch:', sample[0].shape, sample[1].shape)\n",
        "\n",
        "\n",
        "class MyDense(layers.Layer):\n",
        "    # to replace standard layers.Dense()\n",
        "    def __init__(self, inp_dim, outp_dim):\n",
        "        super(MyDense, self).__init__()\n",
        "\n",
        "        self.kernel = self.add_variable('w', [inp_dim, outp_dim])\n",
        "        # self.bias = self.add_variable('b', [outp_dim])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        x = inputs @ self.kernel\n",
        "        return x\n",
        "\n",
        "class MyNetwork(keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyNetwork, self).__init__()\n",
        "\n",
        "        self.fc1 = MyDense(32*32*3, 256)\n",
        "        self.fc2 = MyDense(256, 128)\n",
        "        self.fc3 = MyDense(128, 64)\n",
        "        self.fc4 = MyDense(64, 32)\n",
        "        self.fc5 = MyDense(32, 10)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param inputs: [b, 32, 32, 3]\n",
        "        :param training:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x = tf.reshape(inputs, [-1, 32*32*3])\n",
        "        # [b, 32*32*3] => [b, 256]\n",
        "        x = self.fc1(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        # [b, 256] => [b, 128]\n",
        "        x = self.fc2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        # [b, 128] => [b, 64]\n",
        "        x = self.fc3(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        # [b, 64] => [b, 32]\n",
        "        x = self.fc4(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        # [b, 32] => [b, 10]\n",
        "        x = self.fc5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "network = MyNetwork()\n",
        "network.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "network.fit(train_db, epochs=15, validation_data=test_db, validation_freq=1)\n",
        "\n",
        "network.evaluate(test_db)\n",
        "network.save_weights('ckpt/weights.ckpt')\n",
        "del network\n",
        "print('saved to ckpt/weights.ckpt')\n",
        "\n",
        "\n",
        "network = MyNetwork()\n",
        "network.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "network.load_weights('ckpt/weights.ckpt')\n",
        "print('loaded weights from file.')\n",
        "network.evaluate(test_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "cVg8Uj07xqF9",
        "outputId": "5c44f649-2691-4816-a3cd-e26231533779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "(128, 80) (128,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc211b53255f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'reduce_max'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers,layers,datasets\n",
        "\n",
        "#the most frequest words\n",
        "total_words = 10000\n",
        "max_review_len = 80\n",
        "batchsz = 128\n",
        "embedding_len = 100\n",
        "(x_train,y_train),(x_test,y_test) = datasets.imdb.load_data(num_words=total_words)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train,max_review_len)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test,max_review_len)\n",
        "\n",
        "db_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(1000).batch(batchsz,drop_remainder=True)\n",
        "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batchsz,drop_remainder=True)\n",
        "\n",
        "sample = next(iter(db_train))\n",
        "print(sample[0].shape,sample[1].shape)\n",
        "print(tf.reduce_max(y_train),tf.reduce_min(y_train))\n",
        "\n",
        "\n",
        "class MyRNN(keras.Model):\n",
        "\n",
        "  def __init__(self,units):\n",
        "    super(MyRNN,self).__init__()\n",
        "    #transform text to embedding representation\n",
        "    #[b,100] => [b,80,100]\n",
        "    self.embedding = layers.embedding(total_words,embedding_len,input_len=max_review_len)\n",
        "\n",
        "    #[b,64]\n",
        "    self.state0 = [tf.zeros([batchsz,units])]\n",
        "\n",
        "    #[b,64,100], h_dim:64\n",
        "    #RNNï¼šcell1, cell2, cell3\n",
        "    #SimpleRNN\n",
        "    self.rnn_cell0 = layers.SimpleRNNCell(units,dropuout=0.2)\n",
        "\n",
        "    #fc\n",
        "    self.outlayer = layers.Dence(1)\n",
        "\n",
        "  def call(self,input,traning=None):\n",
        "    #[b,80]\n",
        "    x = input\n",
        "    #embedding:[b,80] => [b,80,100]\n",
        "    x = self.embedding(x)\n",
        "    #RNNcell compute\n",
        "    #[b,80,100] => [b,64]\n",
        "    state = self.state\n",
        "    for word in tf.unstack(x,axis=1): # word: [b,100]\n",
        "      (out,state1) = self.rnn_cell0(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "gTQ_PEIe4eCE",
        "outputId": "46be5d9a-2169-459d-e35e-f583824d9e68"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-cc62f5202400>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    out[x,y] = tf.reduce_sum(inp[,x:x+3,y:y+3] * self.w) + self.b\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,optimizers,datasets\n",
        "\n",
        "class BasicBlock(layers.layer):\n",
        "  def __init__(self,filter_num,stride=1):\n",
        "    super(BasicBlock,self).__init__()\n",
        "    self.conv1 = layers.conv(filter_num,(3,3),stride=stride,padding='same')\n",
        "    self.bn1 = layers.BatchNormalization()\n",
        "    self.relu = layers.Activation('relu')\n",
        "\n",
        "    self.conv1 = layers.conv(filter_num,(3,3),stride=1,padding='same')\n",
        "    self.bn1 = layers.BatchNormalization()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets,optimizers,layers,Sequential\n",
        "\n",
        "tf.random.set_seed(2345)\n",
        "\n",
        "conv_layers=[\n",
        "             layers.Conv2D(64,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.Conv2D(64,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
        "\n",
        "             layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
        "\n",
        "             layers.Conv2D(256,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.Conv2D(256,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
        "\n",
        "             layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
        "\n",
        "             layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
        "             layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same')\n",
        "\n",
        "]\n",
        "def pre_data(x,y):\n",
        "  x = tf.cast(x,dtype=tf.float32) / 255.\n",
        "  y = tf.cast(y,dtype=tf.int32)\n",
        "  return x,y\n",
        "\n",
        "(x,y),(x_test,y_test) = datasets.cifar100.load_data()\n",
        "y = tf.squeeze(y,axis=1)\n",
        "y_test = tf.squeeze(y_test,axis=1)\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "train_db = train_db.shuffle(1000).map(pre_data).batch(64)\n",
        "\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
        "test_db = test_db.map(pre_data).batch(64)\n",
        "\n",
        "def main():\n",
        "  \n",
        "\n",
        "  conv_net = Sequential(conv_layers)\n",
        "  \n",
        "  '''\n",
        "  x = tf.random.normal([4,32,32,3])\n",
        "  out = conv_net(x)\n",
        "  '''\n",
        "  fc_net = Sequential([\n",
        "              layers.Dense(256,activation=tf.nn.relu),\n",
        "              layers.Dense(128,activation=tf.nn.relu),\n",
        "              layers.Dense(100,activation=None)])\n",
        "  \n",
        "  conv_net.build(input_shape=[None,32,32,3])\n",
        "  fc_net.build(input_shape=[None,512])\n",
        "  optimizer = optimizers.Adam(lr=1e-4)\n",
        "\n",
        "  variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
        "  for epoch in range(10):\n",
        "\n",
        "    for step,(x,y) in enumerate(train_db):\n",
        "      with tf.GradientTape() as tape:\n",
        "        out = conv_net(x)\n",
        "        out = tf.reshape(out,[-1,512])\n",
        "        logits = fc_net(out)\n",
        "        y_onehot = tf.one_hot(y,depth=100)\n",
        "        loss = tf.losses.categorical_crossentropy(y_onehot,logits,from_logits=True)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "      grads = tape.gradient(loss,variables)\n",
        "      optimizer.apply_gradients(zip(grads,variables))\n",
        "\n",
        "      if step%100 == 0:\n",
        "        print(epoch,step,'loss:',loss.numpy())\n",
        "\n",
        "    total_num = 0\n",
        "    total_correct = 0\n",
        "    for x,y in test_db:\n",
        "      out = conv_net(x)\n",
        "      out = tf.reshape(out,[-1,512])\n",
        "      logits = fc_net(out)\n",
        "      prob = tf.nn.softmax(logits,axis=1)\n",
        "      pred = tf.argmax(prob, axis=1)\n",
        "      pred = tf.cast(pred, dtype=tf.int32)\n",
        "\n",
        "      correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
        "      correct = tf.reduce_sum(correct)\n",
        "\n",
        "      total_num += x.shape[0]\n",
        "      total_correct += int(correct)\n",
        "\n",
        "    acc = total_correct / total_num\n",
        "    print(epoch, 'acc:', acc)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "metadata": {
        "id": "fu4uOK2A9YIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04wcqreTkrP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a20807d-1211-4804-a5b1-a9ede1203eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 17 14:04:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    59W / 149W |   4377MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1GQLFEgnZ8rK5UY35SWLbqPk_Hfkgntcr",
      "authorship_tag": "ABX9TyOTs9kaIcY/9w65tAFg3IUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}